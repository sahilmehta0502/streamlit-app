from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, StructField, StringType, FloatType

# âœ… Initialize Spark Session with Kafka Support
spark = SparkSession.builder \
    .appName("ImageMetadataStreaming") \
    .config("spark.sql.shuffle.partitions", "2") \
    .getOrCreate()

# âœ… Define Schema for Incoming Data
schema = StructType([
    StructField("file_name", StringType(), True),
    StructField("image_format", StringType(), True),
    StructField("resolution", StringType(), True),
    StructField("size_kb", FloatType(), True)
])

# âœ… Read Stream from Kafka
streaming_df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "image_metadata") \
    .option("startingOffsets", "earliest") \
    .load()

# âœ… Convert Kafka Value (Binary) to JSON
json_df = streaming_df.select(from_json(col("value").cast("string"), schema).alias("data")).select("data.*")

# âœ… Streaming Queries

# ðŸ”¹ Query 1: Count Images by Format (Real-time)
query1 = json_df.groupBy("image_format").count()

# ðŸ”¹ Query 2: Find Largest Image (Real-time)
query2 = json_df.orderBy("size_kb", ascending=False).limit(1)

# ðŸ”¹ Query 3: Find Smallest Image (Real-time)
query3 = json_df.orderBy("size_kb", ascending=True).limit(1)

# âœ… Output Streaming Results to Console
query1.writeStream.format("console").outputMode("complete").start()
query2.writeStream.format("console").outputMode("complete").start()
query3.writeStream.format("console").outputMode("complete").start()

# âœ… Wait for Stream to Run
spark.streams.awaitAnyTermination()
